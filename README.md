# Language-Modeling
- Language modeling is the problem of predicting the next token in a sequence given the previous N tokens.  For example:

- <s> nlp 243 is the best </s>
 0   1   2  3   4   5    6
 
- Built both encoder only and decoder only transformer for unsupervised learning on the Penn Treebank (text-only) dataset.

## Task 
- Compute the perplexity based on each sentence in the test set.
- Perplexity is a metric that quantifies the model's uncertainty or surprise when predicting the next word in a sequence.



